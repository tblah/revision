# 3.3.1 - Problem Solving
## Information Hiding & Abstraction
### *Be familiar with the concept of abstraction as the modeling of a complex system, only including the essential details*

**Abstraction** - representation that is arrived at by removing all unnecessary details

**Interface** - a boundary between the implementation of a system and the world that uses it. It provides and abstraction of the entity behind the boundary thus separating the methods of external communication from internal operation.

**Information Hiding** - hiding design details behind a standard interface.

There are several reasons for information hiding:
+ Different objects can have identical interfaces.
+ Sharing a common interface among many different objects means that users of the objects do not need to be retrained when changing from using one object to using another.
+ To use a module or object, a user needs to have no knowledge of its internal design.
+ The internal design can be kept secret.
+ One module may be replaced by another module with an identical interface or manner in which the module's function is carried out.
+ Changes are made easier because changes are local to modules.

## Comparing Algorithms, Big-O Notation & Order of Complexity

**Algorithm** - a sequence of unambiguous instructions for solving a problem. It can be represented as a Turing machine program.

**Computational Complexity of an Algorithm** -  how economical an algorithm is with time and space

**Basic Operation** - The operation that contributes most to an algorithm's running time.

**Order of growth** - by what factor the execution time increases when the size of the input increases

**Order of Complexity** - an approximation as to the complexity of a problem by looking at the order of growth of the basic operation (see big-O notation)

### *Understand that algorithms can be compared by expressing their complexity as a function relative to the size of the problem.*
Types of complexity:
+ Time Complexity - the time to complete the task under different situations
+ Memory Complexity - the amount of memory required to solve the problem

Measures of complexity:
+ Best Case Complexity - the complexity when running an algorithm with an ideal input (usually a small input) e.g. for an O(n) algorithm this is equal to the overhead for a single operation (effectively O(a))
+ Average Case Complexity - the average complexity as calculated from all possible inputs. E.g. for an O(n) algorithm this would be n/2
+ Worst Case Complexity - the maximum complexity that is possible from any input.

### *Understand that some algorithms are more efficient time-wise than other algorithms.*

For example doing a linear search over a very large data set has greater worst case time complexity than a hash table lookup on the same data because in the worst case, the linear search would have to process every item in the dataset but the hash table would just have t calculate and lookup a hash.

### *Understand that some algorithms are more space-efficient than other algorithms.*

For example a linear search has lower memory complexity than a hash table because the hash table has to maintain space for the whole range of the hash function whereas the linear search can have no memory overhead at all.

### *Understand order of complexity and big-O notation in linear, polynomial and exponential time.*

O(a)    - constant complexity (e.g. queries in hash tables). This means that it always takes a constant time to perform an operation, no matter the data size.

O(n)    - linear complexity (e.g. finding the end of a singly linked list). This means that the ordr of complexity is proportional to the length of the data input.

O(n^a)  - polynomial complexity (e.g. two nested loops is O(n^2), three nested loops is O(n^3)). This means that the ordr of complexity is proportional to the length of data of input to the power of a constant.

O(a^n)  - exponential complexity (e.g. very bad things). This means that the order of complexity is proportional to a constant to the power of the size of the input data.

## Solvable, Non-Solvable & Intractable Problems
*Be aware that software and hardware resent limitations to solving problems.*

*Understand that some algorithms involve too many steps to be solvable in a reasonable amount of time (The Traveling Salesman Problem) in the general case but that a heuristic approach can provide a solution for specific cases.*

*The unsolvable problem of determining whether any program will eventually stop given particular input (halting problem).*

Hardware can has a limited rate at which it can execute software instructions. It can also only store so much data at a time. This is what makes the complexity of algorithms important: the computation must be completed in a reasonable amount of time.

**Tractable Problem** - A problem for which at-least one algorithm exists which can solve the problem within a reasonable amount of time (e.g. sorting through a list).

**Intractable Problem** - A problem for which there is no known algorithm which will solve the problem in a reasonable amount of time (at most polynomial) but theoretical algorithms exist to solve the problem e.g. breaking RSA public key cryptography by factoring very large numbers can theoretically be done however it could take thousands of years on existing hardware.

**Uncomputable Problem** - A problem for which an algorithm cannot be written to solve the problem at all. For example working out if a program will halt given an input is impossible without executing the program to see (this is called the Halting Problem).

**Decision Problem**  - A problem which has a boolean answer.

**Uncescidable Problem** - A decision problem which is uncomputable.

### Solving Intractable Problems
While a problem may be intractable in the general case, in some special cases a heuristic may be used to find a solution.

**Heuristic** - Describes an approach that uses know-how and experience to make *informed guesses* or *trial and error* that assists in finding a polynomial (or better) time solution to an intractable algorithmic problem.

## Finite State Machines, Turing Machines and the Universal Machine
*Abstract model of the Turing Machine and the Universal Machine.*

**Finite State Machine** - An FSM consists of a set of input symbols (input alphabet) and if it produces output, a set of output symbols (output alphabet), a finite set of states and a transition function that maps a state-symbol pair (current state and input symbol) to a state (next state) and possibly generates an output (depending upon the type of FSM).

**Transition Function** - maps an input symbol and a current state to an output symbol and the next state.

**Transition table** - tabulates a number of transition functions.

**Deterministic FSM** - a FSM that has just one next state per input symbol.

**Non-deterministic FSM** - a FSM that may have several possible next states for each pair of state and input symbol.

**Halting state** - a state that has no outgoing transition.

**Accepting state** - a state which the FSM can end execution in.

A FSM represents a system as a set of states, the transitions between the states along with the associated inputs and outputs drawn from the machine's alphabet of symbols. One state is designated the start state.

**Meanly Machine** an FSM that determines its outputs from the present state and from the inputs.

**Moore Machine** an FSM that determines its outputs from the present state only.

**Finite State Automation (FSA)** - an FSM which produces no output while running: just a boolean response when it finishes running. These are used to solve decision problems.

**Universal machine** - A universal machine is a machine capable of simulating any other machine.

**Turing machine** - an FSM that controls one or more tapes, where at least one tape is of unbounded length (infinitely long).

**Power of a Turing machine** - any decision probem which can be solved on any machine can be solved on a Turing machine. Also all other types of computing have an equivalent form as a Turing machine.

*Draw and interpret state transition diagrams for finite state machines with no output (automata) and with output (Mealy and Moore machines).*

**State transition diagram** - a directed graph whose nodes represent the states in an FSM. An edge leading from state s to the state t is called a transition and is labeled with a symbolic code e.g. a|b. The a part of the label is called the transition's trigger and denotes the input symbol. The b part is optional and denotes the output from this transition.

## Regular Expressions
*Form simple regular expressions for string manipulation and matching.*

**Regular Language** - any language that a FSM will accept.

**Regular Expression** - a notation for defining all the valid stings of a formal language or a special text string for describing a search pattern.

### Metacharichters

*a*  a regular expression which will match just a

*ab*  a regular expression which will match the concatenation of ab in tht order.

*a*\*  a regular expression which will match *zero or more* a's.

*a+*  a regular expression which will match *one or more* a's.

*a?*  a regular expression which will match *zero or one* b's.

*a|b*  a regular expression which will match a or b.

*(ab)**  zero or more occurrences of ab. The brackets set order of precedence (concatination is normally last).

*[abc]*  a or b or c.

*a\+*  escape the special character + so that it can be used literally to mean 'a+'.

*^*  beginning of a line.

*$*  end of a line.

*.*  any non whitespace character.

*[a-z0-9A-Z]*   any alphanumeric character: the hyphen when used inside [] will specify a range.

## Backus-Naur Form (BNF)
*Be able to check language syntax by referring to BNF or syntax diagrams.*

**Backus-Nour form (BNF)** - a notation for expressing the rules for constructing valid strings in a regular language.

**Syntax** - is the rules by which words and symbols are combined to form language structures (expressions, statements, etc).

### Notation

< identifier\> ::= < other identifier> | constant | "string constant" | < identifier3>hi

This would mean that < identifier> can comprise of a valid < other identifier> or the value constant or the value "string constant" or any valid < identifier3> concatenated with the constant "hi".

Identifiers can also be recursive:

< digit> ::= 1|2|3|4|5|6|7|8|9|0

< positive integer> ::= < digit> | < digit>< positive integer>

This allows positive integer to be comprised of any natural number of digits. This is the only way of doing this in BNF as BNF has no iteration.

## Reverse Polish Notation
*Be able to convert simple infix notation to reverse Polish notation and vice versa.*

Reverse polish notation is a way to express mathematical expressions on a first in last out data structure without having to store brackets. For this a stack is normally used.

The format for RPN is to have two numerical operands followed by an operation. These are in the order that they are pop-ed off the stack. All three are replaced with the result of the expression.

e.g.

12+34+* = (1+2)*(3+4)

### Important: maintain order of numbers/symbols in the expression when converting either way
